{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is aimed to generate the dictionary for comparison of networks between runs. As such it is semi active, as for a finished project this shouldnt be needed\n",
    "\n",
    "#### similar structure to most of my notebooks - first cell imports various modules, sets variable name and has a list of subjejcts and runs, next cell loads in graph_dict which is used to construct comp_dict, the main loop if very rough both in style and what it contains, may be edited a lot when more direction is decided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell imports the necessary packages - sets variables to point to string keys - sets the sub case and control\n",
    "# dictionaries as well as creating lists of global node and othe metrics\n",
    "\n",
    "# this cell imports the necessary packages - sets variables to point to string keys - sets the sub case and control\n",
    "# dictionaries as well as creating lists of global node and othe metrics\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# set variables that will point to dictionary keys - with the type/output will be in the dict, note these \n",
    "# are not based on the output of the analagous networkx function but how I have written into dict\n",
    "\n",
    "\n",
    "#data/matrices\n",
    "graph = \"graph\" #networkx graph basically dictionaries all the way down\n",
    "data = \"data\" #pandas data frame \n",
    "bin_mat = \"binary_matrix\" #numpy array \n",
    "bin_df = \"binary_df\"\n",
    "data_numpy = \"data_as_numpy_array\" #numpy array\n",
    "norm_lap = \"normalised_laplacian\" #numpy array?\n",
    "\n",
    "#global metrics \n",
    "gl_eff = \"global_efficiency\" #float\n",
    "loc_eff = \"local_efficiency\" #float\n",
    "clus_coef = \"clusetering_coefficent\" #float\n",
    "ave_path_length = \"ave_path_length\" #float\n",
    "clique_num = \"clique_number\" #int (poss float but always integer valued)\n",
    "ass_coef = \"assortativity_coefficient\" #float\n",
    "transitivity = \"transitivity\" #float\n",
    "rich_club_coeff = \"rich_club_coefficient\" #currently not working would be float\n",
    "sw_sigma = \"\" #currently not working would be float\n",
    "sw_gamma = \"\" #currently not working would be float\n",
    "ave_strength = \"average_node_strength\" #float\n",
    "no_edges = \"number_of_edges\" #int\n",
    "ave_deg = \"average_degree\" #int\n",
    "\n",
    "#individual node/edge metrics and dicts\n",
    "deg = \"degree\" #dictionary - nodes as key degree as values \n",
    "bet_cent = \"betweenness_centrality\" # dictionary - nodes as key betweeness as values \n",
    "edge_bet_cent = \"edge_betweenness_centrality\" # dictionary - edge tuple as key edgebetweeness centrality as values \n",
    "eigen_cent = \"eigenvector_centrality\" # dictionary - nodes as eigenvalue centrality as values \n",
    "deg_cent = \"degree_centrality\" # dictionary - nodes as key degree centrality as values \n",
    "strength = \"node_strength\"  # dictionary - nodes as key strength as values \n",
    "\n",
    "#other\n",
    "deg_hist = \"degree_histogram\" #list - index is degree value is how many nodes with that degree \n",
    "norm_lap_spec = \"normalised_laplacian_spectrum\" #list of floats?\n",
    "\n",
    "\n",
    "#subject dictionary with a list of runs - corresponds to how connectome file is saved for now hand written \n",
    "sub_dict = {\"NENAH004\" : [\"orig\", \"run2\"], \"NENAH010\" :  [\"orig\", \"run2\"], \"NENAH011\" :  [\"orig\", \"run2\"],\n",
    "                \"NENAH012\" : [\"orig\", \"run2\"], \"NENAH015\" : [\"orig\", \"run2\"], \"NENAH016\" : [\"orig\", \"run2\"], \n",
    "                \"NENAH022\" : [\"orig\", \"run2\"], \"NENAH024\" : [\"orig\", \"run2\"], \"NENAHC002\" : [\"orig\", \"run2\", \"run3\"], \n",
    "                \"NENAHC003\" : [\"orig\", \"run2\",\"run3\"], \"NENAHC010\" : [\"orig\", \"run2\", \"run3\"],\n",
    "                \"NENAHC013\" : [\"orig\", \"run2\", \"run3\"], \"NENAHC016\" : [\"orig\", \"run2\", \"run3\"],\n",
    "                \"NENAHC018\" : [\"orig\", \"run2\", \"run3\"], \"NENAHC020\": [\"orig\", \"run2\", \"run3\"],\n",
    "                \"NENAHC025\" : [\"orig\", \"run2\"]\n",
    "           }\n",
    "\n",
    "case_dict = {\"NENAH004\" : [\"orig\", \"run2\"], \"NENAH010\" :  [\"orig\", \"run2\"], \"NENAH011\" :  [\"orig\", \"run2\"],\n",
    "                \"NENAH012\" : [\"orig\", \"run2\"], \"NENAH015\" : [\"orig\", \"run2\"], \"NENAH016\" : [\"orig\", \"run2\"], \n",
    "                \"NENAH022\" : [\"orig\", \"run2\"], \"NENAH024\" : [\"orig\", \"run2\"]\n",
    "            }\n",
    "\n",
    "control_dict = { \"NENAHC002\" : [\"orig\", \"run2\", \"run3\"],  \"NENAHC003\" : [\"orig\", \"run2\", \"run3\"], \n",
    "                \"NENAHC010\" : [\"orig\", \"run2\", \"run3\"],\"NENAHC013\" : [\"orig\", \"run2\", \"run3\"],\n",
    "                \"NENAHC016\" : [\"orig\", \"run2\", \"run3\"], \"NENAHC018\" : [\"orig\", \"run2\", \"run3\"],\n",
    "                \"NENAHC020\": [\"orig\", \"run2\", \"run3\"], \"NENAHC025\" : [\"orig\", \"run2\",]\n",
    "             }\n",
    "\n",
    "#create the lists which might be usuefull in automating data frame creation\n",
    "#note these should be strings\n",
    "global_metrics = [gl_eff, loc_eff, clus_coef, ave_path_length, clique_num,\n",
    "                  transitivity, ave_strength, no_edges, ave_deg] \n",
    "\n",
    "node_metrics = [deg, strength, bet_cent, eigen_cent, deg_cent]\n",
    "\n",
    "other_metrics = [deg_hist, norm_lap_spec, edge_bet_cent,]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open main graph dict which is used as data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('graph_dict.pickle', 'rb') as handle:\n",
    "     graph_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the main loop that generates the comp_dict, the idea is that each subject has a dictionary with various keys and vlaues - often one for each run, but lots caluclated as differences - so far this is not taking into account 3rd runs - will look to do something more statistically significant with them, - one thing to noe is that bin_diff is a binary matrix (technically pandas data frame) taking vallue of one wherever the corresponding edge is present in exactly one run, and 0 elsewhere - hence 'multiplying elementwise (how dataframes natural multiplication is defined where appropriate)' either runs original matrix by it will give a matrix of only the unique edges in that run, this is used at multiple thresholds, \n",
    "\n",
    "I may heavily reduce the number of items stored in the comp_dict as many are simple caluclations based off of others\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the main generator of the dictionary used to comapre across runs this is very rough and could be cleaned up when I \n",
    "# have a better idea at what I want to compare, for now \n",
    "\n",
    "# note - need to go through and check no double counting is happening, i believe it is for various node \n",
    "# strength categories\n",
    "\n",
    "comp_dict = {}\n",
    "thresholds = [0.05, 0.02, 0.01, 0.005, 0.001, 0.0005, 0.0001]\n",
    "for sub in sub_dict:\n",
    "    comp_dict[sub] = {}\n",
    "    comp_dict[sub]['orig_data'] = graph_dict[f'{sub}orig'][\"data\"]\n",
    "    comp_dict[sub]['run2_data'] = graph_dict[f'{sub}run2'][\"data\"]\n",
    "    comp_dict[sub]['bin_diff']  = abs(graph_dict[f'{sub}orig'][\"binary_df_0\"] - graph_dict[f'{sub}run2'][\"binary_df_0\"])\n",
    "    comp_dict[sub]['edges_different'] = comp_dict[sub]['bin_diff'].sum().sum() / 2\n",
    "    comp_dict[sub]['orig_edges'] = graph_dict[f'{sub}orig'][\"number_of_edges\"]\n",
    "    comp_dict[sub]['run2_edges'] = graph_dict[f'{sub}run2'][\"number_of_edges\"]\n",
    "    comp_dict[sub]['average_edges'] = 0.5 * (comp_dict[sub]['orig_edges'] + comp_dict[sub]['run2_edges'])\n",
    "    comp_dict[sub]['percentage_edge_diff'] = 100* (comp_dict[sub]['edges_different'] / comp_dict[sub]['average_edges'])\n",
    "    comp_dict[sub]['res_orig_df']= comp_dict[sub]['bin_diff'] * comp_dict[sub]['orig_data']\n",
    "    comp_dict[sub]['res_orig_bin_df'] = (comp_dict[sub]['res_orig_df']> 0).astype(np.int_)\n",
    "    comp_dict[sub]['res_run2_df'] = comp_dict[sub]['bin_diff'] * comp_dict[sub]['run2_data']\n",
    "    comp_dict[sub]['res_run2_bin_df'] = (comp_dict[sub]['res_run2_df'] > 0).astype(np.int_)\n",
    "    comp_dict[sub]['orig_unique_edges'] = 0.5 * (comp_dict[sub]['res_orig_bin_df'].sum().sum())\n",
    "    comp_dict[sub]['run2_unique_edges'] = 0.5 * (comp_dict[sub]['res_run2_bin_df'].sum().sum())\n",
    "    comp_dict[sub]['orig_unique_edges_pct'] = 100 * (comp_dict[sub]['orig_unique_edges'] / comp_dict[sub]['orig_edges'])\n",
    "    comp_dict[sub]['run2_unique_edges_pct'] = 100 * (comp_dict[sub]['run2_unique_edges'] / comp_dict[sub]['run2_edges'])\n",
    "    comp_dict[sub]['orig_unique_node_strength'] = comp_dict[sub]['res_orig_df'].sum()\n",
    "    comp_dict[sub]['orig_unique_node_strength_pct'] = 100 * (comp_dict[sub]['orig_unique_node_strength'] / comp_dict[sub]['orig_data'].sum())\n",
    "    comp_dict[sub]['run2_unique_node_strength'] = comp_dict[sub]['res_run2_df'].sum()\n",
    "    comp_dict[sub]['run2_unique_node_strength_pct'] = 100 * (comp_dict[sub]['run2_unique_node_strength'] / comp_dict[sub]['run2_data'].sum())\n",
    "    comp_dict[sub]['orig_unique_tot_strength'] = comp_dict[sub]['res_orig_df'].sum().sum()\n",
    "    comp_dict[sub]['orig_unique_tot_strength_pct'] = 100 * (comp_dict[sub]['orig_unique_tot_strength'] / comp_dict[sub]['orig_data'].sum().sum())\n",
    "    comp_dict[sub]['run2_unique_tot_strength'] = comp_dict[sub]['res_run2_df'].sum().sum()\n",
    "    comp_dict[sub]['run2_unique_tot_strength_pct'] = 100 * (comp_dict[sub]['run2_unique_tot_strength'] / comp_dict[sub]['run2_data'].sum().sum())\n",
    "    for thresh in thresholds:\n",
    "        comp_dict[sub][f'unique_orig_{thresh}_bin'] = (comp_dict[sub]['res_orig_df']> thresh).astype(np.int_)\n",
    "        comp_dict[sub][f'unique_orig_{thresh}'] = comp_dict[sub]['orig_data'] * comp_dict[sub][f'unique_orig_{thresh}_bin']\n",
    "        comp_dict[sub][f'unique_run2_{thresh}_bin' ]= (comp_dict[sub]['res_run2_df'] > thresh).astype(np.int_)\n",
    "        comp_dict[sub][f'unique_run2_{thresh}' ]= comp_dict[sub]['run2_data']* comp_dict[sub][f'unique_run2_{thresh}_bin' ]\n",
    "        comp_dict[sub][f'no_edges_unique_orig_{thresh}']= 0.5 * (comp_dict[sub][f'unique_orig_{thresh}_bin'].sum().sum())\n",
    "        comp_dict[sub][f'no_edges_unique_run2_{thresh}' ]= 0.5 * (comp_dict[sub][f'unique_run2_{thresh}_bin'].sum().sum())\n",
    "        comp_dict[sub][f'percentage_unique_orig_{thresh}'] = comp_dict[sub][f'no_edges_unique_orig_{thresh}'] / comp_dict[sub]['orig_edges']\n",
    "        comp_dict[sub][f'percentage_unique_run2_{thresh}'] = comp_dict[sub][f'no_edges_unique_run2_{thresh}'] / comp_dict[sub]['run2_edges']\n",
    "        comp_dict[sub][f'diff_by_{thresh}'] = (abs(graph_dict[f'{sub}orig'][data] - graph_dict[f'{sub}run2'][data]) > thresh).astype(np.int_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['NENAH004', 'NENAH010', 'NENAH011', 'NENAH012', 'NENAH015', 'NENAH016', 'NENAH022', 'NENAH024', 'NENAHC002', 'NENAHC003', 'NENAHC010', 'NENAHC013', 'NENAHC016', 'NENAHC018', 'NENAHC020', 'NENAHC025'])\n"
     ]
    }
   ],
   "source": [
    "print(comp_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "898"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_dict['NENAHC025']['bin_diff'].sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_dict['NENAHC025']['unique_run2_0.005_bin'].sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub in sub_dict :\n",
    "    print(sub)\n",
    "    print(comp_dict[sub]['percentage_edge_diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub in sub_dict :\n",
    "    print(sub)\n",
    "    print(comp_dict[sub]['orig_unique_tot_strength_pct'])\n",
    "    print(comp_dict[sub]['run2_unique_tot_strength_pct'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two cells are for saving and loading (via pickle) and will use space below for testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('comp_dict.pickle', 'wb') as handle:\n",
    "    pickle.dump(comp_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('comp_dict.pickle', 'rb') as handle:\n",
    "    comp_dict = pickle.load(handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
