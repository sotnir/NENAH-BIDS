{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation of main graph dictionary and various specific data frames \n",
    "\n",
    "This notebook is the main notebook for generating the graphs and metrics from the raw connectome files, it produces \n",
    "and saves (pickles) ' graph_dict ' which includes a dictionary for each subject (run) which contains the data itself, the graph generated from networkx, various global metrics and dictionaries of various node based measures. The intent is to use this graph dicionary as the main data source of analysis - other data frames and dictionaries are then to be created from this. This should involve as many computationally expensive steps as possible to avoid them later.\n",
    "\n",
    "Ideally I will then convert into a proper script and break up into functions but right now I am focused on getting usable code for analysis on the smaller multi run data \n",
    "\n",
    "For now it is organised as follows - \n",
    "1) an introductory cell to import and set various lists and variables to point to strings that will be dictionary keys\n",
    "\n",
    "2) a main loop that creates and fills the graph dict with the various metrics (this isn't set in stone, more metrics wwill be added, some will be discarded\n",
    "\n",
    "3) a cell to re-structure the dictionary labels for node based measures to be more human readable, and less prone to mistakes when converting and working with data as a matrix\n",
    "\n",
    "4) cells to pickle and unpickle the main dictionary\n",
    "\n",
    "5) a few cells to generate node feature data frames that are suitable to be saved as csv to be easier to call in analysis \n",
    "\n",
    "6) a couple of measures like degree hist and laplacian spectrum which I havent decided how to save and use yet\n",
    "\n",
    "Note the structure is in (Subject)(run) this can cause confusion and conflict - however it is currently the most flexible way (I can think of) to organise as I am still analysisng the effects of rerunning tractography, it should be fairly simple to adapt the code to either not include (run) or standadise one run - the former is likely to be how I handle it once we have decided on a way forward with consistent tractography. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell imports the necessary packages - sets variables to point to string keys - sets the sub case and control\n",
    "# dictionaries as well as creating lists of global node and othe metrics\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# set variables that will point to dictionary keys - with the type/output will be in the dict, note these \n",
    "# are not based on the output of the analagous networkx function but how I have written into dict\n",
    "\n",
    "\n",
    "#data/matrices\n",
    "graph = \"graph\" #networkx graph basically dictionaries all the way down\n",
    "data = \"data\" #pandas data frame \n",
    "bin_mat = \"binary_matrix\" #numpy array \n",
    "bin_df = \"binary_df\"\n",
    "data_numpy = \"data_as_numpy_array\" #numpy array\n",
    "norm_lap = \"normalised_laplacian\" #numpy array?\n",
    "\n",
    "#global metrics \n",
    "gl_eff = \"global_efficiency\" #float\n",
    "loc_eff = \"local_efficiency\" #float\n",
    "clus_coef = \"clusetering_coefficent\" #float\n",
    "ave_path_length = \"ave_path_length\" #float\n",
    "clique_num = \"clique_number\" #int (poss float but always integer valued)\n",
    "ass_coef = \"assortativity_coefficient\" #float\n",
    "transitivity = \"transitivity\" #float\n",
    "rich_club_coeff = \"rich_club_coefficient\" #currently not working would be float\n",
    "sw_sigma = \"\" #currently not working would be float\n",
    "sw_gamma = \"\" #currently not working would be float\n",
    "ave_strength = \"average_node_strength\" #float\n",
    "no_edges = \"number_of_edges\" #int\n",
    "ave_deg = \"average_degree\" #int\n",
    "\n",
    "#individual node/edge metrics and dicts\n",
    "deg = \"degree\" #dictionary - nodes as key degree as values \n",
    "bet_cent = \"betweenness_centrality\" # dictionary - nodes as key betweeness as values \n",
    "edge_bet_cent = \"edge_betweenness_centrality\" # dictionary - edge tuple as key edgebetweeness centrality as values \n",
    "eigen_cent = \"eigenvector_centrality\" # dictionary - nodes as eigenvalue centrality as values \n",
    "deg_cent = \"degree_centrality\" # dictionary - nodes as key degree centrality as values \n",
    "strength = \"node_strength\"  # dictionary - nodes as key strength as values \n",
    "\n",
    "#other\n",
    "deg_hist = \"degree_histogram\" #list - index is degree value is how many nodes with that degree \n",
    "norm_lap_spec = \"normalised_laplacian_spectrum\" #list of floats?\n",
    "\n",
    "\n",
    "#subject dictionary with a list of runs - corresponds to how connectome file is saved for now hand written \n",
    "sub_dict = {\"NENAH004\" : [\"orig\", \"run2\"], \"NENAH010\" :  [\"orig\", \"run2\"], \"NENAH011\" :  [\"orig\", \"run2\"],\n",
    "                \"NENAH012\" : [\"orig\", \"run2\"], \"NENAH015\" : [\"orig\", \"run2\"], \"NENAH016\" : [\"orig\", \"run2\"], \n",
    "                \"NENAH022\" : [\"orig\", \"run2\"], \"NENAH024\" : [\"orig\", \"run2\"], \"NENAHC002\" : [\"orig\", \"run2\", \"run3\"], \n",
    "                \"NENAHC003\" : [\"orig\", \"run2\",\"run3\"], \"NENAHC010\" : [\"orig\", \"run2\", \"run3\"],\n",
    "                \"NENAHC013\" : [\"orig\", \"run2\", \"run3\"], \"NENAHC016\" : [\"orig\", \"run2\", \"run3\"],\n",
    "                \"NENAHC018\" : [\"orig\", \"run2\", \"run3\"], \"NENAHC020\": [\"orig\", \"run2\", \"run3\"],\n",
    "                \"NENAHC025\" : [\"orig\", \"run2\"]\n",
    "           }\n",
    "\n",
    "case_dict = {\"NENAH004\" : [\"orig\", \"run2\"], \"NENAH010\" :  [\"orig\", \"run2\"], \"NENAH011\" :  [\"orig\", \"run2\"],\n",
    "                \"NENAH012\" : [\"orig\", \"run2\"], \"NENAH015\" : [\"orig\", \"run2\"], \"NENAH016\" : [\"orig\", \"run2\"], \n",
    "                \"NENAH022\" : [\"orig\", \"run2\"], \"NENAH024\" : [\"orig\", \"run2\"]\n",
    "            }\n",
    "\n",
    "control_dict = { \"NENAHC002\" : [\"orig\", \"run2\", \"run3\"],  \"NENAHC003\" : [\"orig\", \"run2\", \"run3\"], \n",
    "                \"NENAHC010\" : [\"orig\", \"run2\", \"run3\"],\"NENAHC013\" : [\"orig\", \"run2\", \"run3\"],\n",
    "                \"NENAHC016\" : [\"orig\", \"run2\", \"run3\"], \"NENAHC018\" : [\"orig\", \"run2\", \"run3\"],\n",
    "                \"NENAHC020\": [\"orig\", \"run2\", \"run3\"], \"NENAHC025\" : [\"orig\", \"run2\",]\n",
    "             }\n",
    "\n",
    "#create the lists which might be usuefull in automating data frame creation\n",
    "#note these should be strings\n",
    "global_metrics = [gl_eff, loc_eff, clus_coef, ave_path_length, clique_num,\n",
    "                  transitivity, ave_strength, no_edges, ave_deg] \n",
    "\n",
    "node_metrics = [deg, strength, bet_cent, eigen_cent, deg_cent]\n",
    "\n",
    "other_metrics = [deg_hist, norm_lap_spec, edge_bet_cent,]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I may need to change the main loop below to reflect how I have now made comp_dict, ie include residules and weighted and binary outputs at thresholds - however that will involve non-trivial looping\n",
    "\n",
    "### this cell is the main gerative loop - generating main dict that all other data frames can be generated from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell is the main gerative loop - generating main dict that all other data frames\n",
    "# can be generated from\n",
    "\n",
    "\n",
    "#to test main loop \n",
    "# sub_dict = {\"NENAH004\" : [\"orig\", \"run2\"], \"NENAHC018\" : [\"orig\", \"run2\", \"run3\"] }\n",
    "\n",
    "#initialise the main dictionary of the graphs - this will be pickled saved and used to create lots of different \n",
    "#data frames and various things,\n",
    "graph_dict={}\n",
    "\n",
    "# initialise various input parameters for main functions - these are tempoary depending on how analysis goes \n",
    "binary_thresholds = [0, 0.05, 0.02, 0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005, 0.00001]\n",
    "#note 0.0001 was first order of magnitude to display difference, 0.1 was too high to be meaningful, \n",
    "# havent tried non powers of 10 yet \n",
    "\n",
    "#main loop - loops over subjects and then runs according to the dictionary, and then loops over runs\n",
    "# for for each run a dictionary is created with the various metrics graphs and related data\n",
    "# this is using a file structure not like bids atm but was easier for me to set up - all conncectomes are labelled\n",
    "# by sub idea in one folder - this will be changed into a raw data folder or to use the bids data structure for now \n",
    "# I will keep it like this as it is still clear and usable\n",
    "for sub in sub_dict :\n",
    "    file =\"_10M_sift__Connectome_\"\n",
    "    for run in sub_dict[sub] :\n",
    "#         print(run)\n",
    "        #initialise the dictionary for this subjects specific run of tractography\n",
    "        graph_dict[f'{sub}{run}']={}\n",
    "        print(f'{sub}{file}{run}.csv')\n",
    "        \n",
    "#         load in data for subjectrun and create the grapj \n",
    "        df = pd.read_csv(f'{sub}{file}{run}.csv', header=None)\n",
    "        G = nx.from_pandas_adjacency(df)\n",
    "\n",
    "    # add these into subjectrun dictionary\n",
    "    \n",
    "        graph_dict[f'{sub}{run}'][\"graph\"] = G\n",
    "        graph_dict[f'{sub}{run}'][\"data\"] = df\n",
    "        graph_dict[f'{sub}{run}'][data_numpy] = pd.DataFrame.to_numpy(df)\n",
    "           # this runs through the numpy array checking if the value is above binary threshold - assigning one if true 0 if not\n",
    "            \n",
    "# calculate and add to dictionary various graph mesures, the majority are basic networkx functions some I \n",
    "#     \n",
    "        graph_dict[f'{sub}{run}'][no_edges] = nx.number_of_edges(G)\n",
    "        graph_dict[f'{sub}{run}'][deg] = dict(nx.degree(G))\n",
    "        graph_dict[f'{sub}{run}'][ave_deg] = sum(graph_dict[f'{sub}{run}'][deg].values()) / (len(graph_dict[f'{sub}{run}'][graph]))\n",
    "        graph_dict[f'{sub}{run}'][strength] = dict(nx.degree(G, weight=\"weight\"))\n",
    "        graph_dict[f'{sub}{run}'][ave_strength] = sum(graph_dict[f'{sub}{run}'][strength].values()) / (len(graph_dict[f'{sub}{run}'][graph]))\n",
    "        graph_dict[f'{sub}{run}'][gl_eff] = nx.global_efficiency(G)\n",
    "        graph_dict[f'{sub}{run}'][loc_eff] = nx.local_efficiency(G)\n",
    "        graph_dict[f'{sub}{run}'][clus_coef] = nx.average_clustering(G)\n",
    "        graph_dict[f'{sub}{run}'][ave_path_length] = nx.average_shortest_path_length(G)\n",
    "        graph_dict[f'{sub}{run}'][clique_num] = nx.graph_clique_number(G)\n",
    "        graph_dict[f'{sub}{run}'][transitivity] = nx.transitivity(G)\n",
    "        graph_dict[f'{sub}{run}'][norm_lap] = nx.normalized_laplacian_matrix(G).toarray()\n",
    "        graph_dict[f'{sub}{run}'][norm_lap_spec] = nx.normalized_laplacian_spectrum(G)\n",
    "        graph_dict[f'{sub}{run}'][deg_hist] = nx.degree_histogram(G)\n",
    "        graph_dict[f'{sub}{run}'][bet_cent] = nx.betweenness_centrality(G)\n",
    "        graph_dict[f'{sub}{run}'][edge_bet_cent] = nx.edge_betweenness_centrality(G)\n",
    "        graph_dict[f'{sub}{run}'][eigen_cent] = nx.eigenvector_centrality_numpy(G, weight=\"weight\")\n",
    "        graph_dict[f'{sub}{run}'][deg_cent] = nx.degree_centrality(G)\n",
    "        for threshold in binary_thresholds :\n",
    "            graph_dict[f'{sub}{run}'][bin_mat +f'_{str(threshold)}'] = (graph_dict[f'{sub}{run}'][data_numpy] > threshold).astype(np.int_)\n",
    "            graph_dict[f'{sub}{run}'][bin_df +f'_{str(threshold)}'] = (graph_dict[f'{sub}{run}'][data] > threshold).astype(np.int_)\n",
    "#         graph_dict[f'{sub}{run}'][rich_club_coeff] = nx.rich_club_coefficient(G) - cant get this to wor\n",
    "#         graph_dict[f'{sub}{run}'][ass_coef] = \\\n",
    "#         graph_dict[f'{sub}{run}'][small world omega]\n",
    "#         graph_dict[f'{sub}{run}'][small world sigma]\n",
    "#         print(f'{sub}{run}')\n",
    "#         print(graph_dict[f'{sub}{run}'][norm_lap])\n",
    "\n",
    "# to add - fixed rich and small - degree sequence manually deg_hist awkward - strength distribution?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then as a check see which keys have been created for a subject - note the current name structure being 'subrun'\n",
    "It is worth using this cell to inspect some values as well "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph_dict[\"NENAH004orig\"].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell relables all the 'node metrics' dictionaries to have keys of 'node x' instead of simply 'x' this reads better \n",
    "both when presented as tables and will avoid potentially missing conversion errors as will no longer just be a number\n",
    "which may have not been converted to various matrices correctly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# re label node attribute keys as \"node x\" instead of x\n",
    "\n",
    "for sub in sub_dict :\n",
    "    for run in sub_dict[sub] :\n",
    "        for met in node_metrics :\n",
    "            for i in range(len(graph_dict[f'{sub}{run}'][met])):\n",
    "                graph_dict[f'{sub}{run}'][met][\"node \" + str(i)] = graph_dict[f'{sub}{run}'][met].pop(i)\n",
    "                \n",
    "                \n",
    "# check relabel as expected on a particular metric \n",
    "# print(graph_dict[\"NENAH004orig\"][strength])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The next cell pickles and saves the main graph_dict,\n",
    "###### The one after loads the file 'graph_dict.pickle' which should be in the expected format and is assigned as 'graph_dict' this is for testing later steps without having to re-generate the whole graph_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle and save the main graph dict generated abbove - note pickle is not secure so may have to think\n",
    "# of alternative ways of storing the graphs and complicated structures along with subjects and run info\n",
    "\n",
    "with open('graph_dict.pickle', 'wb') as handle:\n",
    "    pickle.dump(graph_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load (unpickle the graph _dict) - note pickle is not secure so may have to think\n",
    "# of alternative ways of storing the graphs and complicated structures along with subjects and run info\n",
    "\n",
    "with open('graph_dict.pickle', 'rb') as handle:\n",
    "     graph_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the next few cells are used to create and save node based measures into sepreate dict and df\n",
    "\n",
    "# degree - first loop creates data frame and saves for all subjects, the second\n",
    "# for only case subjects from case_dict and the third only controls from control_dict\n",
    "# all 3 dicts should have same format {sub: [list of runs]}\n",
    "\n",
    "#for test loop only\n",
    "\n",
    "\n",
    "deg_dict = {}\n",
    "for sub in sub_dict:\n",
    "    for run in sub_dict[sub]:\n",
    "        deg_dict[f'{sub}{run}'] = graph_dict[f'{sub}{run}'][deg]\n",
    "\n",
    "deg_df = pd.DataFrame.from_dict(deg_dict, orient=\"index\")\n",
    "deg_df.index.name = \"sub-id\"\n",
    "deg_df.to_csv(f'{deg}.csv')\n",
    "\n",
    "# print(deg_dict)\n",
    "\n",
    "# initialise and fill dictionary with degree dict for each subject\n",
    "deg_case_dict = {}\n",
    "for sub in case_dict:\n",
    "    for run in case_dict[sub]:\n",
    "        deg_case_dict[f'{sub}{run}'] = graph_dict[f'{sub}{run}'][deg]\n",
    "\n",
    "# create and save data ftame\n",
    "deg_case_df = pd.DataFrame.from_dict(deg_case_dict, orient=\"index\")\n",
    "deg_case_df.index.name = \"sub-id\"\n",
    "deg_case_df.to_csv(f'{deg}_case.csv')\n",
    "\n",
    "\n",
    "deg_control_dict = {}\n",
    "for sub in control_dict:\n",
    "    for run in control_dict[sub]:\n",
    "        deg_control_dict[f'{sub}{run}'] = graph_dict[f'{sub}{run}'][deg]\n",
    "\n",
    "\n",
    "deg_control_df = pd.DataFrame.from_dict(deg_control_dict, orient=\"index\")\n",
    "deg_control_df.index.name = \"sub-id\"\n",
    "deg_control_df.to_csv(f'{deg}_control.csv')\n",
    "\n",
    "\n",
    "# print(deg_control_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#node strength\n",
    "#  first loop creates data frame and saves for all subjects, the second\n",
    "# for only case subjects from case_dict and the third only controls from control_dict\n",
    "# all 3 dicts should have same format {sub: [list of runs]}\n",
    "\n",
    "\n",
    "\n",
    "strength_dict ={}\n",
    "for sub in sub_dict :\n",
    "    for run in sub_dict[sub] :\n",
    "        strength_dict[f'{sub}{run}'] = graph_dict[f'{sub}{run}'][strength] \n",
    "\n",
    "strength_df = pd.DataFrame.from_dict(strength_dict, orient = \"index\")\n",
    "strength_df.index.name = \"sub-id\"\n",
    "strength_df.to_csv(f'{strength}.csv')\n",
    "\n",
    "strength_case_dict = {}\n",
    "for sub in case_dict:\n",
    "    for run in case_dict[sub]:\n",
    "        strength_case_dict[f'{sub}{run}'] = graph_dict[f'{sub}{run}'][strength]\n",
    "\n",
    "\n",
    "strength_case_df = pd.DataFrame.from_dict(strength_case_dict, orient=\"index\")\n",
    "strength_case_df.index.name = \"sub-id\"\n",
    "strength_case_df.to_csv(f'{strength}_case.csv')\n",
    "\n",
    "\n",
    "strength_control_dict = {}\n",
    "for sub in control_dict:\n",
    "    for run in control_dict[sub]:\n",
    "        strength_control_dict[f'{sub}{run}'] = graph_dict[f'{sub}{run}'][strength]\n",
    "\n",
    "strength_control_df = pd.DataFrame.from_dict(strength_control_dict, orient=\"index\")\n",
    "strength_control_df.index.name = \"sub-id\"\n",
    "strength_control_df.to_csv(f'{strength}_control.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge betweeness centrality \n",
    "# first loop creates data frame and saves for all subjects, the second\n",
    "# for only case subjects from case_dict and the third only controls from control_dict\n",
    "# all 3 dicts should have same format {sub: [list of runs]}\n",
    "\n",
    "\n",
    "edge_bet_cent_dict ={}\n",
    "for sub in sub_dict :\n",
    "    for run in sub_dict[sub] :\n",
    "        edge_bet_cent_dict[f'{sub}{run}'] = graph_dict[f'{sub}{run}'][edge_bet_cent] \n",
    "\n",
    "            \n",
    "edge_bet_cent_df = pd.DataFrame.from_dict(edge_bet_cent_dict, orient = \"index\")\n",
    "edge_bet_cent_df.index.name = \"sub-id\"\n",
    "edge_bet_cent_df.to_csv(f'{edge_bet_cent}.csv')\n",
    "\n",
    "edge_bet_cent_case_dict = {}\n",
    "for sub in case_dict:\n",
    "    for run in case_dict[sub]:\n",
    "        edge_bet_cent_case_dict[f'{sub}{run}'] = graph_dict[f'{sub}{run}'][edge_bet_cent]\n",
    "\n",
    "edge_bet_cent_case_df = pd.DataFrame.from_dict(edge_bet_cent_case_dict, orient=\"index\")\n",
    "edge_bet_cent_case_df.index.name = \"sub-id\"\n",
    "edge_bet_cent_case_df.to_csv(f'{edge_bet_cent}_case.csv')\n",
    "\n",
    "edge_bet_cent_control_dict = {}\n",
    "for sub in control_dict:\n",
    "    for run in control_dict[sub]:\n",
    "        edge_bet_cent_control_dict[f'{sub}{run}'] = graph_dict[f'{sub}{run}'][edge_bet_cent]\n",
    "\n",
    "edge_bet_cent_control_df = pd.DataFrame.from_dict(edge_bet_cent_dict, orient=\"index\")\n",
    "edge_bet_cent_control_df.index.name = \"sub-id\"\n",
    "edge_bet_cent_control_df.to_csv(f'{edge_bet_cent}_control.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#betweeness centrality\n",
    "# first loop creates data frame and saves for all subjects, the second\n",
    "# for only case subjects from case_dict and the third only controls from control_dict\n",
    "# all 3 dicts should have same format {sub: [list of runs]}\n",
    "\n",
    "# note only re label keys on first loop as the subsequent loops take from this key changed dict\n",
    "\n",
    "bet_cent_dict ={}\n",
    "for sub in sub_dict :\n",
    "    for run in sub_dict[sub] :\n",
    "        bet_cent_dict[f'{sub}{run}'] = graph_dict[f'{sub}{run}'][bet_cent] \n",
    "\n",
    "            \n",
    "bet_cent_df = pd.DataFrame.from_dict(bet_cent_dict, orient = \"index\")\n",
    "bet_cent_df.index.name = \"sub-id\"\n",
    "bet_cent_df.to_csv(f'{bet_cent}.csv')\n",
    "\n",
    "bet_cent_case_dict = {}\n",
    "for sub in case_dict:\n",
    "    for run in case_dict[sub]:\n",
    "        bet_cent_case_dict[f'{sub}{run}'] = graph_dict[f'{sub}{run}'][bet_cent]\n",
    "\n",
    "bet_cent_case_df = pd.DataFrame.from_dict(bet_cent_case_dict, orient=\"index\")\n",
    "bet_cent_case_df.index.name = \"sub-id\"\n",
    "bet_cent_case_df.to_csv(f'{bet_cent}_case.csv')\n",
    "\n",
    "bet_cent_control_dict = {}\n",
    "for sub in control_dict:\n",
    "    for run in control_dict[sub]:\n",
    "        bet_cent_control_dict[f'{sub}{run}'] = graph_dict[f'{sub}{run}'][bet_cent]\n",
    "\n",
    "bet_cent_control_df = pd.DataFrame.from_dict(bet_cent_control_dict, orient=\"index\")\n",
    "bet_cent_control_df.index.name = \"sub-id\"\n",
    "bet_cent_control_df.to_csv(f'{bet_cent}_control.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eigenvector centrality \n",
    "# first loop creates data frame and saves for all subjects, the second\n",
    "# for only case subjects from case_dict and the third only controls from control_dict\n",
    "# all 3 dicts should have same format {sub: [list of runs]}\n",
    "\n",
    "\n",
    "eigen_cent_dict ={}\n",
    "for sub in sub_dict :\n",
    "    for run in sub_dict[sub] :\n",
    "        eigen_cent_dict[f'{sub}{run}'] = graph_dict[f'{sub}{run}'][eigen_cent] \n",
    "\n",
    "\n",
    "eigen_cent_df = pd.DataFrame.from_dict(eigen_cent_dict, orient = \"index\")\n",
    "eigen_cent_df.index.name = \"sub-id\"\n",
    "eigen_cent_df.to_csv(f'{eigen_cent}.csv')\n",
    "\n",
    "eigen_cent_case_dict ={}\n",
    "for sub in case_dict :\n",
    "    for run in case_dict[sub] :\n",
    "        eigen_cent_case_dict[f'{sub}{run}'] = graph_dict[f'{sub}{run}'][eigen_cent] \n",
    "\n",
    "\n",
    "eigen_cent_case_df = pd.DataFrame.from_dict(eigen_cent_case_dict, orient = \"index\")\n",
    "eigen_cent_case_df.index.name = \"sub-id\"\n",
    "eigen_cent_case_df.to_csv(f'{eigen_cent}_case.csv')\n",
    "\n",
    "eigen_cent_control_dict ={}\n",
    "for sub in control_dict :\n",
    "    for run in control_dict[sub] :\n",
    "        eigen_cent_control_dict[f'{sub}{run}'] = graph_dict[f'{sub}{run}'][eigen_cent] \n",
    "\n",
    "\n",
    "eigen_cent_control_df = pd.DataFrame.from_dict(eigen_cent_control_dict, orient = \"index\")\n",
    "eigen_cent_control_df.index.name = \"sub-id\"\n",
    "eigen_cent_control_df.to_csv(f'{eigen_cent}_control.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# degree centrality\n",
    "# first loop creates data frame and saves for all subjects, the second\n",
    "# for only case subjects from case_dict and the third only controls from control_dict\n",
    "# all 3 dicts should have same format {sub: [list of runs]}\n",
    "\n",
    "deg_cent_dict ={}\n",
    "for sub in sub_dict :\n",
    "    for run in sub_dict[sub] :\n",
    "        deg_cent_dict[f'{sub}{run}'] = graph_dict[f'{sub}{run}'][deg_cent]\n",
    "            \n",
    "deg_cent_df = pd.DataFrame.from_dict(deg_cent_dict, orient = \"index\")\n",
    "deg_cent_df.index.name = \"sub-id\"\n",
    "deg_cent_df.to_csv(f'{deg_cent}.csv')\n",
    "\n",
    "deg_cent_case_dict ={}\n",
    "for sub in case_dict :\n",
    "    for run in case_dict[sub] :\n",
    "        deg_cent_case_dict[f'{sub}{run}'] = graph_dict[f'{sub}{run}'][deg_cent] \n",
    "            \n",
    "deg_cent_case_df = pd.DataFrame.from_dict(deg_cent_case_dict, orient = \"index\")\n",
    "deg_cent_case_df.index.name = \"sub-id\"\n",
    "deg_cent_case_df.to_csv(f'{deg_cent}_case.csv')\n",
    "\n",
    "deg_cent_control_dict ={}\n",
    "for sub in control_dict :\n",
    "    for run in control_dict[sub] :\n",
    "        deg_cent_control_dict[f'{sub}{run}'] = graph_dict[f'{sub}{run}'][deg_cent] \n",
    "            \n",
    "deg_cent_control_df = pd.DataFrame.from_dict(deg_cent_control_dict, orient = \"index\")\n",
    "deg_cent_control_df.index.name = \"sub-id\"\n",
    "deg_cent_control_df.to_csv(f'{deg_cent}_control.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#produce and save data frame of global metrics and seperate case and controls\n",
    "# first loop creates data frame and saves for all subjects, the second\n",
    "# for only case subjects from case_dict and the third only controls from control_dict\n",
    "# all 3 dicts should have same format {sub: [list of runs]}\n",
    "\n",
    "glo_met = \"global_metrics\"\n",
    "glo_met_dict ={}\n",
    "#this loops through all subjects and runs stored in sub_dict - this should be all subjects \n",
    "for sub in sub_dict :\n",
    "    for run in sub_dict[sub] :\n",
    "        glo_met_dict[f'{sub}{run}'] ={}\n",
    "        for met in global_metrics :\n",
    "            glo_met_dict[f'{sub}{run}'][met] = graph_dict[f'{sub}{run}'][met]\n",
    "            \n",
    "glo_met_df = pd.DataFrame.from_dict(glo_met_dict, orient = \"index\")\n",
    "glo_met_df.index.name = \"sub-id\"\n",
    "glo_met_df.to_csv(f'{glo_met}.csv')         \n",
    "\n",
    "#same as above but only for case subjects - supplied in same format as sub_dict\n",
    "glo_met_case_dict = {}\n",
    "for sub in case_dict :\n",
    "    for run in case_dict[sub] :\n",
    "        glo_met_case_dict[f'{sub}{run}'] ={}\n",
    "        for met in global_metrics :\n",
    "            glo_met_case_dict[f'{sub}{run}'][met] = graph_dict[f'{sub}{run}'][met]\n",
    "            \n",
    "glo_met_case_df = pd.DataFrame.from_dict(glo_met_case_dict, orient = \"index\")\n",
    "glo_met_case_df.index.name = \"sub-id\"\n",
    "glo_met_case_df.to_csv(f'{glo_met}_case.csv') \n",
    "\n",
    "#same as above but only for control subjects - supplied in same format as sub_dict\n",
    "glo_met_control_dict = {}\n",
    "for sub in control_dict :\n",
    "    for run in control_dict[sub] :\n",
    "        glo_met_control_dict[f'{sub}{run}'] ={}\n",
    "        for met in global_metrics :\n",
    "            glo_met_control_dict[f'{sub}{run}'][met] = graph_dict[f'{sub}{run}'][met]\n",
    "            \n",
    "glo_met_control_df = pd.DataFrame.from_dict(glo_met_control_dict, orient = \"index\")\n",
    "glo_met_control_df.index.name = \"sub-id\"\n",
    "glo_met_control_df.to_csv(f'{glo_met}_control.csv') \n",
    "\n",
    "# print(glo_met_df.head())\n",
    "# print(glo_met_case_df.head())\n",
    "# print(glo_met_control_df.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two cells are currently unsaved - the first is the degree histogram - however I want to code this in a different way myself as the current networkx implementation is strange, the second is the laplacian spectrum, need to think of a nice way to save this and check if dict_from csv (or whatever similar functions are called) would work as expected with a dict list structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#produce dictionary of the degree histogram \n",
    "# first loop creates for all subjects, the second only case subjects from case_dict and the\n",
    "# third only controls from control_dict all 3 dicts should have same format {sub: [list of runs]}\n",
    "\n",
    "deg_hist_dict = {}\n",
    "for sub in sub_dict :\n",
    "    for run in sub_dict[sub] :\n",
    "        deg_hist_dict[f'{sub}{run}'] = graph_dict[f'{sub}{run}'][deg_hist]\n",
    "        \n",
    "deg_hist_case_dict = {}\n",
    "for sub in case_dict :\n",
    "    for run in case_dict[sub] :\n",
    "        deg_hist_case_dict[f'{sub}{run}'] = graph_dict[f'{sub}{run}'][deg_hist]\n",
    "\n",
    "deg_hist_control_dict = {}\n",
    "for sub in control_dict :\n",
    "    for run in control_dict[sub] :\n",
    "        deg_hist_control_dict[f'{sub}{run}'] = graph_dict[f'{sub}{run}'][deg_hist]\n",
    "        \n",
    "# print(deg_hist_control_dict)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#produce the normed laplacian spectrum dictionary \n",
    "# first loop creates for all subjects, the second only case subjects from case_dict and the\n",
    "# third only controls from control_dict all 3 dicts should have same format {sub: [list of runs]}\n",
    "\n",
    "\n",
    "norm_lap_spec_dict = {}\n",
    "for sub in sub_dict :\n",
    "    for run in sub_dict[sub] :\n",
    "        norm_lap_spec_dict[f'{sub}{run}'] = graph_dict[f'{sub}{run}'][norm_lap_spec]\n",
    "        \n",
    "norm_lap_spec_case_dict = {}\n",
    "for sub in case_dict :\n",
    "    for run in case_dict[sub] :\n",
    "        norm_lap_spec_case_dict[f'{sub}{run}'] = graph_dict[f'{sub}{run}'][norm_lap_spec]\n",
    "        \n",
    "norm_lap_spec_control_dict = {}\n",
    "for sub in control_dict :\n",
    "    for run in control_dict[sub] :\n",
    "        norm_lap_spec_control_dict[f'{sub}{run}'] = graph_dict[f'{sub}{run}'][norm_lap_spec]\n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below here cells are only for testing, development and notes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# degree sequence, test degree sequence code to implement to plot - degree_hist seems more awkard to plot \n",
    "#  need to think hard - most of the current metrics only use binary nature of an edge - as weight isnt distance \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe adjust some algortihms that requirer re using node metrics \n",
    "# eg - if i have already calculated norm-lap_spec use that for algebraic connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['diff_matrix_0.05', 'diff_df_0.05', 'diff_matrix_0.02', 'diff_df_0.02', 'diff_matrix_0.01', 'diff_df_0.01', 'diff_matrix_0.005', 'diff_df_0.005', 'diff_matrix_0.001', 'diff_df_0.001', 'diff_matrix_0.0005', 'diff_df_0.0005', 'diff_matrix_0.0001', 'diff_df_0.0001'])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create differenct matrices/dataframe for various thresholds - this is currently diffucnt, currently using different notebook and comp_dict\n",
    "diff_thresh = [0.05, 0.02, 0.01, 0.005, 0.001, 0.0005, 0.0001]\n",
    "diff_dict = {}\n",
    "for sub in sub_dict :\n",
    "    diff_dict[sub] ={}\n",
    "    for thresh in diff_thresh :\n",
    "        diff_dict[sub][f'diff_matrix_{str(thresh)}'] = (abs(graph_dict[f'{sub}orig'][data_numpy] - graph_dict[f'{sub}run2'][data_numpy]) > thresh).astype(np.int_)        \n",
    "        diff_dict[sub][f'diff_df_{str(thresh)}'] = (abs(graph_dict[f'{sub}orig'][data] - graph_dict[f'{sub}run2'][data]) > thresh).astype(np.int_)\n",
    "        \n",
    "diff_dict['NENAHC003'].keys()\n",
    "# (graph_dict[f'{sub}{run}'][data_numpy] > threshold).astype(np.int_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('diff_dict.pickle', 'wb') as handle:\n",
    "    pickle.dump(diff_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# need to create minimum (maximum) spanning trees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum spanning tree for each sub for each run create maximum spanning tree - if it is deemed worthy of study \n",
    "# will be incoperated into main loop, hopefully should be consistent between runs and may serve as a sort of average too\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
